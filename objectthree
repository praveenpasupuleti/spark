package com.vanguard.hdm.transaction.tdata_items

import java.util.Properties

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Row
import org.apache.spark.sql.functions.udf
import org.apache.spark.sql.types.DecimalType
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StructType
import org.apache.spark.storage.StorageLevel

import com.vanguard.hdm.cmn.utilities.PropertiesReader
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.TimestampType
import java.sql.Timestamp
import java.text.SimpleDateFormat

object DivisionCode_TA_Driver {

   private var DEBUG : Boolean = false
   
   val workflow_id = "DivisionCode_TA"

  def main(args: Array[String]): Unit = {

    if (args.length < 1) {
      System.err.println("Missing properties file")
      System.exit(1)
    }

    val APP_NAME = getAppName()

    val spark = SparkSession
      .builder()
      .enableHiveSupport()
      .appName(APP_NAME)
      .config("spark.sql.parquet.writeLegacyFormat", "true")
      .getOrCreate()

    val properties_dict = PropertiesReader.get_properties_dict(args(0), args)
    properties_dict.setProperty("workflow_id", workflow_id)
    
    val finalDataFrame = calculate_combined_TA_df(spark, properties_dict)
    finalDataFrame.persist(StorageLevel.DISK_ONLY)
    writeFiles(finalDataFrame, properties_dict)
    finalDataFrame.unpersist()
    spark.stop()
  }
   
  def calculate_combined_TA_df(spark: SparkSession, properties_dict: Properties): DataFrame = {
    DEBUG = properties_dict.getProperty("debug_flag").toString().toBoolean    
    DivisionCode_II.calculate_final_df_and_saveAsFile(spark, properties_dict)
    DivisionCode_TA.calculate_final_TA_df_and_saveAsFile(spark, properties_dict)
    
    return collect_total_data_and_convert_df(spark, properties_dict)
  }
  
  def collect_total_data_and_convert_df(spark: SparkSession, properties_dict: Properties) : DataFrame = {
     val workflow_id = properties_dict.getProperty("workflow_id")
     val data_item_dir = properties_dict.getProperty("data_item_dir")
     val schema  = StructType(Array(
            StructField( "po_id", IntegerType, true),
            StructField( "acct_id", DecimalType(15,0) , true),
            StructField( "acct_posn_id", DecimalType(15,0), true),
            StructField( "vast_acct_no", StringType, true),
            StructField( "port_id", StringType, true),
            StructField( "mntry_divsn_cd", StringType, true),
            StructField( "posn_serv_sgmnt_cd", StringType, true),
            StructField( "efftv_bgn_dt", TimestampType, true),
            StructField( "efftv_end_dt", TimestampType, true)
            ))
    
    val sc = spark.sparkContext
    val iiData = sc.textFile(s"/user/hadoop/${data_item_dir}/${workflow_id}/division_ii").map(x => x.split(",")).map(x=> Row (Integer.valueOf(x(0)), BigDecimal(x(1)), BigDecimal(x(2)), x(3), x(4), x(5), x(6), createTimeStamp(x(7)), createTimeStamp(x(8)))).keyBy(x => x(2))
    iiData.persist(StorageLevel.DISK_ONLY)
    var taData = sc.textFile(s"/user/hadoop/${data_item_dir}/${workflow_id}/division_ta").map(x => x.split(",")).map(x=> Row (Integer.valueOf(x(0)), BigDecimal(x(1)), BigDecimal(x(2)), x(3), x(4), x(5), x(6), createTimeStamp(x(7)), createTimeStamp(x(8)))).keyBy(x => x(2)).subtractByKey(iiData)
    taData.persist(StorageLevel.DISK_ONLY)
    
    val sqlContext = spark.sqlContext
    var finalCombinedTADataFrame = sqlContext.createDataFrame(iiData.union(taData).values, schema)
    iiData.unpersist()
    taData.unpersist()
    
    finalCombinedTADataFrame.persist(StorageLevel.DISK_ONLY)
    finalCombinedTADataFrame = finalCombinedTADataFrame.dropDuplicates()
    if (DEBUG) {
    	 finalCombinedTADataFrame.show()
    }
    
    return finalCombinedTADataFrame
  }
  def createTimeStamp(time:String): Timestamp = {
    val sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
    val date = sdf.parse(time)
    val timeStamp = new Timestamp(date.getTime())
    return timeStamp
  }
  
  def writeFiles ( finalDF:DataFrame, properties_dict:Properties ) = {
    finalDF.coalesce(4)
    finalDF.write.parquet(properties_dict.getProperty("data_item_dir") + "/" + properties_dict.getProperty( "workflow_id" ) + "/" + "division_full_ta")
  }
  
  def getAppName(): String = { 
    return "Combined Division Code - TA & II scala application"
  }
}
